<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Rise of the Google Assistant</title>
    <link href="./style.css" rel="stylesheet" type="text/css">
</head>

<body>
    <header>
        <h1><a href="index.html">The Rise of the Google Assistant</a></h1>
        <nav>
            <table>
                <tr>
                    <th><a href="voice-search.html">Google Voice Search</a></th>
                    <th><a href="now.html">Google Now</a></th>
                    <th><a href="allo.html">Google Allo</a></th>
                    <th><a href="home.html">Google Home</a></th>
                    <th><a href="sdk.html">Google Assistant SDK</a></th>
                </tr>
                <tr>
                    <td>June 14, 2011</td>
                    <td>July 9, 2012</td>
                    <td>September 21, 2016</td>
                    <td>November 4, 2016</td>
                    <td>April 27, 2017</td>
                </tr>
            </table>
        </nav>
    </header>
    <main id="search">
        <h2>Google Voice Search</h2>
        <p>The well-known and rather sophisticated <abbr title="Artificial Intelligence">AI</abbr> known as the
            Google Assistant had a humble beginning as an augmentation to Google Search. With <em>Google Voice
                Search</em>, users are able to prompt the Google search engine with their voice, increasing
            accessibility and ease of use and often decreasing danger. While it is significantly simpler than
            the Google Assistant, its developers tackled many of the same issues faced today. Among them are
            enormous vocabularies, unpredictable noise conditions, and unfamiliar accents. </p>
        <figure>
            <a target="_blank" title="Google Search by Voice: A case study"
                href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36340.pdf"><img
                    src="./img/voice.png"
                    alt="Google Voice Search logo"></a>
            <figcaption>Google, Public domain, via Wikimedia Commons</figcaption>
        </figure>
        <p>The original <em>Google Voice Search</em> used either buttons (physical or on a touch screen) or
            gestures as a trigger. When buttons were used, endpointer software was most often used to recognize
            the end of speech, silence being the key signal. Manual endpoints included a second press of a
            button or a press-and-hold method of recording. The Google Mobile App for IOS included a
            gesture-based trigger activated by an accelerometer when a user brings the phone to his or her ear.
        </p>
        <p>When faced with multiple hypotheses (interpretations) of the userâ€™s speech, the application would
            include what is called the N-Best List, which places the most likely hypothesis at the top. The
            challenge is where to display the list so that it neither hinders the display of results nor is
            difficult to find and use. A solution is the combination of a confidence score and a drop-down
            arrow. The confidence score evaluates how likely the top hypothesis is correct, and the software
            will call attention to the list when that score is low. </p>
    </main>
</body>